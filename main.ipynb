{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8224c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading news from the CNBC web site\n",
    "\n",
    "# key word that news should include\n",
    "querry_word = \"apple\"\n",
    "\n",
    "params = {\n",
    "    \"queryly_key\": \"31a35d40a9a64ab3\",\n",
    "    \"query\": querry_word,\n",
    "    \"endindex\": \"0\",\n",
    "    \"batchsize\": \"100\",\n",
    "    \"callback\": \"\",\n",
    "    \"showfaceted\": \"true\",\n",
    "    \"timezoneoffset\": \"-120\",\n",
    "    \"facetedfields\": \"formats\",\n",
    "    \"facetedkey\": \"formats|\",\n",
    "    \"facetedvalue\":\n",
    "    \"!Press Release|\",\n",
    "    \"needtoptickers\": \"1\",\n",
    "    \"additionalindexes\": \"4cd6f71fbf22424d,937d600b0d0d4e23,3bfbe40caee7443e,626fdfcd96444f28\"\n",
    "}\n",
    "\n",
    "# parts of news article that will be included, in our project we use only Title and Publication Date but Description is also\n",
    "# present as we can train model on description of articles\n",
    "goal = [\"cn:title\", \"_pubDate\", \"description\"]\n",
    "\n",
    "\n",
    "def main(url):\n",
    "    with requests.Session() as req:\n",
    "        allin = []\n",
    "        for page, item in enumerate(range(5000, 20000, 100)):  # to get more or less news change range\n",
    "                                                          # later news in the begging and old news are in the end\n",
    "            print(f\"Extracting Page# {page +1}\")\n",
    "            params[\"endindex\"] = item\n",
    "            r = req.get(url, params=params).json()\n",
    "            for loop in r['results']:\n",
    "                allin.append([loop[x] for x in goal])\n",
    "        new = pd.DataFrame(\n",
    "            allin, columns=[\"Title\", \"date\", \"Description\"])\n",
    "        new.to_csv(\"datasets/data.csv\", index=False)\n",
    "\n",
    "\n",
    "main(\"https://api.queryly.com/cnbc/json.aspx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the news articles data and AAPL stock's data\n",
    "\n",
    "# AAPL.csv is downloaded from Kaggle dataset about the Apple's stock prices\n",
    "a = pd.read_csv(\"datasets/AAPL.csv\")\n",
    "b = pd.read_csv(\"datasets/data.csv\")\n",
    "\n",
    "\n",
    "# since data from CNBC and Kaggle dataset have different format for \"date\" (day-month-year:00:00 and day/month/year)\n",
    "# we change their format and standartize it\n",
    "a['date'] = pd.to_datetime(a['date'], dayfirst=True, utc = False).dt.tz_localize(None).dt.strftime(\"%Y-%m-%d\")\n",
    "b['date'] = pd.to_datetime(b['date'], dayfirst=True, utc = False).dt.tz_localize(None).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# saving cvs files\n",
    "a.to_csv(\"datasets/a.csv\")\n",
    "b.to_csv(\"datasets/b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging 2 cvs files, saving only entries that share same \"date\" value so we have infomation about stock price and\n",
    "# news articles that were published that day regarfing the company\n",
    "merged_df = pd.merge(a, b, on='date')\n",
    "print(a.info(), b.info())\n",
    "\n",
    "# saving merged dataset as \"output.csv\"\n",
    "merged_df.to_csv(\"datasets/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and prepared word embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(inp):\n",
    "\n",
    "  # tokenize the article name\n",
    "  tokens = tokenizer.encode(inp,   return_tensors='pt')\n",
    "\n",
    "  # pass the tokens to model to get sentiment\n",
    "  output = model(tokens)\n",
    "\n",
    "  # pass model output logits through a softmax layer\n",
    "  sentim_scores = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "\n",
    "  # returning sum of values that are multiplied by coeficients: 1 for positive, -1 for neative and 0 for neutaral\n",
    "  # getting in the end following range: numbers closer to 1 means article is most likely positive, closer to -1 most likely negative and closer to 0 is neuteral\n",
    "  return  sentim_scores.detach().numpy()[0][0]*1+sentim_scores.detach().numpy()[0][1]*-1+sentim_scores.detach().numpy()[0][2]*0\n",
    "\n",
    "# creating 'sentiment' column in out csv file. Correlating values are output of sentiment_score function with 'Title' as input\n",
    "merged_df['sentiment'] = merged_df['Title'].apply(lambda x: sentiment_score(x[:150]))\n",
    "merged_df.to_csv(\"datasets/output_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and compile model\n",
    "def create_model(in_shape,output_shape):\n",
    "    model= tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(units = 50,return_sequences=True, input_shape=(in_shape[0],in_shape[1]),kernel_regularizer=tf.keras.regularizers.l1(l1=0.01)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.GRU(units=50, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)),\n",
    "        tf.keras.layers.Dense(output_shape)])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#We are using the close price of the last 60 days to predict the next day's close (this one will be used for pure LSTM)\n",
    "#inp_sp=[60,1]\n",
    "\n",
    "#We take the closing price the volume and sentiment analysis of a newspaper on that day and predict the next day's closing price\n",
    "inp_sp=[3,1]\n",
    "out_sp=1\n",
    "model=create_model(inp_sp,out_sp)\n",
    "\n",
    "#save model\n",
    "model.save(\"predictor5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this file we wil be training the Neural Network\n",
    "\n",
    "#firstly we need to get the training sample and the labels\n",
    "df = pd.read_csv('datasets/output_sentiment.csv')\n",
    "\n",
    "#dataset will be the training sample with shape (n,3,1) where n is the number of rows in /content/output_sentiment .csv\n",
    "average_sentiment_per_day = df.groupby('date')[[\"close\",\"sentiment\",\"volume\"]].mean().reset_index()\n",
    "dataset=average_sentiment_per_day[[\"close\",\"sentiment\",\"volume\"]].values.tolist()\n",
    "\n",
    "#we create the labels, they will be the next close price value for each element in dataset\n",
    "df = pd.read_csv('datasets/a.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#get list of dates from the dataset\n",
    "date_list = average_sentiment_per_day[\"date\"].tolist()\n",
    "next_rows_df = pd.DataFrame()\n",
    "\n",
    "#for each element add to the labels the close price of the next day of that element\n",
    "for date in date_list:\n",
    "    # Find the index of the date\n",
    "    matching_indices = df.index[df['date'] == date].tolist()\n",
    "    if matching_indices:\n",
    "        #make sure the last element is not the last row of the a.csv file other wise there will be no label\n",
    "        if matching_indices[0]<len(df)-1:\n",
    "          next_index = matching_indices[0] + 1\n",
    "          if next_index<len(df):\n",
    "              # Append the next row to the next_rows_df DataFrame\n",
    "              next_rows_df = pd.concat([ next_rows_df, df.iloc[next_index]],axis = 1,  join='outer')\n",
    "              #next_rows_df = next_rows_df.append(df.iloc[next_index])\n",
    "    else:\n",
    "      #remove the element with no label\n",
    "      dataset.pop()\n",
    "\n",
    "\n",
    "next_rows_df = next_rows_df.T\n",
    "correct_tmp=next_rows_df[\"close\"].tolist()\n",
    "\n",
    "#partition dataset into training sample and validation and test sample\n",
    "training_tmp=dataset[:500]+dataset[600:]\n",
    "correct_tmp1=correct_tmp[:500]+correct_tmp[600:]\n",
    "test_sample=dataset[500:600]\n",
    "test_correct=correct_tmp[500:600]\n",
    "indices = list(range(len(correct_tmp1)))\n",
    "random.shuffle(indices)\n",
    "split = int(len(indices) * 0.7)\n",
    "training= [training_tmp[i] for i in indices[:split]]\n",
    "correct= [correct_tmp1[i] for i in indices[:split]]\n",
    "training=tf.constant(training)\n",
    "correct=tf.constant(correct)\n",
    "val= [training_tmp[i] for i in indices[split:]]\n",
    "val_cor= [correct_tmp1[i] for i in indices[split:]]\n",
    "val=tf.constant(val)\n",
    "val_cor=tf.constant(val_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
  
  
    "\n",
    "#train the model on the data and save it (batch_size and epochs can be played around with to find the ideal numbers)\n",
    "history = model.fit(training, correct, epochs=900, batch_size=32, validation_data=(val,val_cor))\n",
    "model.save(\"predictor5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded72ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my kernel kept crashing because of matplotlib so if you experience a similar issue uncomment the next 2 line it might help\n",
    "#it is a specific problem I encountered it might not be the same for everyone\n",
    "#import os\n",  
    "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "#load model\n",
    "model=tf.keras.models.load_model(\"predictor5.keras\")\n",
    "\n",
    "test_sample=dataset[500:600]\n",
    "test_correct=correct_tmp[500:600]\n",
    "\n",
    "\n",
    "#if you want to have day by day prediction as oposed to 30 day prediction comment out previous part and uncomment this part\n",
    "dataset_len=100\n",
    "trainining=[]\n",
    "date=[]\n",
    "for i in range(dataset_len):\n",
    "    date.append(i)\n",
    "prediction=model.predict(test_sample)\n",
    "\n",
    "#plot the predicted prices compared to the actual prices\n",
    "plt.figure()\n",
    "plt.plot(date, prediction, label='predictions')\n",
    "plt.plot(date, test_correct, label='actual')\n",
    "plt.title('prediction vs actual')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate how much money the model makes us on our test sample\n",
    "money=10000\n",
    "for i in range(1,len(prediction)):\n",
    "  #if we predict that the price will go up \"buy\"\n",
    "  estimated_gain=prediction[i][0] - test_correct[i-1]\n",
    "  if estimated_gain>0:\n",
    "    #calclate how much money that day made us\n",
    "    no_share=money/test_correct[i-1]\n",
    "    money=no_share*test_correct[i]\n",
    "\n",
    "#how much money we would have made if the model predicted perfectly\n",
    "money_ideal=10000\n",
    "for i in range(1,len(prediction)):\n",
    "  #if the stock goes up \"buy\"\n",
    "  estimated_gain=test_correct[i]-test_correct[i-1]\n",
    "  if estimated_gain>0:\n",
    "    #calculate how much we make\n",
    "    no_share=money_ideal/test_correct[i-1]\n",
    "    money_ideal=no_share*test_correct[i]\n",
    "\n",
    "print(\"how much we made: \", money)\n",
    "print(\"max amount we could have made if the model predicted perfectly: \",money_ideal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
